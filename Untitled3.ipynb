{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import librosa\n",
    "import torchaudio\n",
    "from feature.feature_prepare import thchs30,MagicData,primewords,ST_CMDS\n",
    "\n",
    "\n",
    "def load_audio(path):\n",
    "    sound, _ = torchaudio.load(path)\n",
    "    sound = sound.numpy().T\n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    return sound\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, label_dic,file_dic, usage = \"train\",transform=None):\n",
    "        self.label_dic = label_dic\n",
    "        self.file_dic = file_dic\n",
    "        self.usage = usage\n",
    "        self.transform = transform\n",
    "        self.audio_conf = dict(sample_rate=16000,\n",
    "                        window_size=0.02,\n",
    "                        window_stride=0.01,\n",
    "                        window=\"hamming\",\n",
    "                        noise_dir=None,\n",
    "                        noise_prob=0.4,\n",
    "                        noise_levels=(0.0, 0.5))\n",
    "        self.normalize = True\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        继承 Dataset 类后,必须重写的一个方法\n",
    "        返回数据集的大小\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(self.file_dic[self.usage])\n",
    "     \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        继承 Dataset 类后,必须重写的一个方法\n",
    "        返回第 idx 个图像及相关信息\n",
    "        :param idx:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        feature_length = 800\n",
    "        file_name = self.file_dic[self.usage][idx]\n",
    "        wav_dir =  self.file_dic[\"file_dic\"]['wav'][file_name]\n",
    "        feature = self.load_wav_feature(wav_dir)\n",
    "        label = self.label_dic[file_name]['code']\n",
    "\n",
    "#         while feature.shape[0] < feature_length:\n",
    "#             feature = np.concatenate((feature, feature), axis=0)\n",
    "#             label.extend(label)\n",
    "        \n",
    "#         feature = np.pad(feature,((0, 1600-feature.shape[0]), (0, 0)),'constant')\n",
    "#         feature = np.reshape(feature, [1, feature.shape[0], feature.shape[1]])\n",
    "        feature = torch.Tensor(feature)\n",
    "\n",
    "        return feature,label,len(label)\n",
    "    \n",
    "    def load_wav_feature(self, file_dir):\n",
    "        y = load_audio(file_dir)\n",
    "\n",
    "        n_fft = int(self.audio_conf[\"sample_rate\"] * self.audio_conf[\"window_size\"])\n",
    "        win_length = n_fft\n",
    "        hop_length = int(self.audio_conf[\"sample_rate\"]  * self.audio_conf[\"window_size\"])\n",
    "        # STFT\n",
    "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                         win_length=win_length, window=self.audio_conf[\"window\"])\n",
    "        spect, phase = librosa.magphase(D)\n",
    "        # S = log(S+1)\n",
    "        spect = np.log1p(spect)\n",
    "        spect = torch.FloatTensor(spect)\n",
    "        if self.normalize:\n",
    "            mean = spect.mean()\n",
    "            std = spect.std()\n",
    "            spect.add_(-mean)\n",
    "            spect.div_(std)\n",
    "\n",
    "        return spect\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def aligin_collate(batch_size):\n",
    "    \"\"\"process variable length labels \"\"\"\n",
    "    wave_list = list()\n",
    "    label_list = list()\n",
    "    length_list = list()\n",
    "    for _, (wave, label, length) in enumerate(batch_size):\n",
    "        wave_list.append(wave)\n",
    "        label_list.extend(label)\n",
    "        length_list.append(length)\n",
    "\n",
    "    stacked_wave = torch.stack(wave_list, dim=0)\n",
    "    label = torch.IntTensor(np.array(label_list))\n",
    "    length = torch.IntTensor(np.array(length_list))\n",
    "\n",
    "    return stacked_wave, label, length\n",
    "\n",
    "def get_data_loader( ):\n",
    "\n",
    "\n",
    "    thchs30_dataset = thchs30(\"/data/jiaxin.gu/jupyter/asr_test/dataset/data_thchs30\",label_file_type = 'trn')\n",
    "    thchs30_dataset.read_label_file()    \n",
    "    ASRDataset_ = ASRDataset(thchs30_dataset.label_dic,thchs30_dataset.file_dic,\"train\")\n",
    "    train_loader = torch.utils.data.DataLoader(ASRDataset_,\n",
    "                                            batch_size=3,\n",
    "                                            collate_fn=aligin_collate,\n",
    "                                            shuffle=True,\n",
    "                                            drop_last=True,\n",
    "                                            num_workers=8)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/jiaxin.gu/jupyter/asr_test/dataset/data_thchs30/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "thchs30_dataset = thchs30(\"/data/jiaxin.gu/jupyter/asr_test/dataset/data_thchs30\",label_file_type = 'trn')\n",
    "thchs30_dataset.read_label_file()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.9414,  3.0051,  4.0349,  ...,  3.6628,  4.0328,  0.7338],\n",
       "         [ 3.8549,  1.4800,  2.1494,  ...,  1.8797,  2.1140,  0.6266],\n",
       "         [ 1.2094,  0.7675,  0.7176,  ..., -0.3736, -0.3496,  0.3540],\n",
       "         ...,\n",
       "         [-0.4184, -0.4353, -0.4386,  ..., -0.4373, -0.4356, -0.4284],\n",
       "         [-0.4187, -0.4370, -0.4383,  ..., -0.4416, -0.4322, -0.4306],\n",
       "         [-0.4164, -0.4399, -0.4301,  ..., -0.4391, -0.4257, -0.4346]]),\n",
       " [198,\n",
       "  1159,\n",
       "  1140,\n",
       "  317,\n",
       "  799,\n",
       "  245,\n",
       "  1039,\n",
       "  58,\n",
       "  345,\n",
       "  443,\n",
       "  239,\n",
       "  75,\n",
       "  331,\n",
       "  1148,\n",
       "  1080,\n",
       "  799,\n",
       "  559,\n",
       "  1039,\n",
       "  342,\n",
       "  1159,\n",
       "  842,\n",
       "  844,\n",
       "  1049,\n",
       "  1221,\n",
       "  1139,\n",
       "  1084,\n",
       "  164,\n",
       "  219,\n",
       "  1064,\n",
       "  8,\n",
       "  1233,\n",
       "  561,\n",
       "  75,\n",
       "  52,\n",
       "  271,\n",
       "  326],\n",
       " 36)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASRDataset_ = ASRDataset(thchs30_dataset.label_dic,thchs30_dataset.file_dic,\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/jiaxin.gu/jupyter/asr_test/dataset/data_thchs30/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fb5c83c70f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/jiaxin.gu/anaconda3/envs/tf/lib/python36.zip/../dataset/train/\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e63ac9b1208c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMagicData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/jupyter/asr_test/feature/feature_prepare.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_dir, label_file, label_file_type)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mlabel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/../dataset/train/TRANS.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             label_file_type = None):\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_file_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_label_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabelsplit\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mlabel_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/asr_test/feature/feature_prepare.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_dir, label_file, label_file_type)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_file_type\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlabel_file_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child_file_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_symbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetSymbolList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/asr_test/feature/feature_prepare.py\u001b[0m in \u001b[0;36mget_child_file_dict\u001b[0;34m(self, root_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m         }\n\u001b[1;32m     69\u001b[0m         \"\"\"\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file_dict_for_different_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_label_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/asr_test/utils/dir_op.py\u001b[0m in \u001b[0;36mget_file_dict_for_different_type\u001b[0;34m(file_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_file_dict_for_different_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mfile_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_files_name_ls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/asr_test/utils/dir_op.py\u001b[0m in \u001b[0;36mget_all_files_name_ls\u001b[0;34m(file_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_files_name_ls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mfile_dir_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdir_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdir_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_child_dir_ls_from_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdir_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mfiles_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_file_name_ls_from_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "MagicData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thchs30,MagicData,primewords,ST_CMDS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
